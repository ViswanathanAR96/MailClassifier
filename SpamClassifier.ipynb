{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read Spam-Ham csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>enron methanol ; meter # : 988291\\r\\nthis is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>hpl nom for january 9 , 2001\\r\\n( see attache...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ham</td>\n",
       "      <td>neon retreat\\r\\nho ho ho , we ' re around to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spam</td>\n",
       "      <td>photoshop , windows , office . cheap . main t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>re : indian springs\\r\\nthis deal is to book t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham   enron methanol ; meter # : 988291\\r\\nthis is ...\n",
       "1   ham   hpl nom for january 9 , 2001\\r\\n( see attache...\n",
       "2   ham   neon retreat\\r\\nho ho ho , we ' re around to ...\n",
       "3  spam   photoshop , windows , office . cheap . main t...\n",
       "4   ham   re : indian springs\\r\\nthis deal is to book t..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get file path\n",
    "root = os.path.dirname( os.path.abspath('SpamClassifier.ipynb'))\n",
    "data_path = \"{0}/spam_ham_dataset.csv\".format(root)\n",
    "#Read csv file\n",
    "data = pd.read_csv(data_path)[['label', 'text']]\n",
    "#Print first five rows of csv\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 5171 datapoints and 1 feature(s) in this dataset. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Dataframe Describe\n",
    "\n",
    "print(\"There are {0} datapoints and {1} feature(s) in this dataset. \\n\".format(data.shape[0],int(data.shape[1])-1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Processing Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stopwords, punctuations and stemm the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rem_punctuations(text):\n",
    "    new_text =  \"\".join([t for t in text if t not in string.punctuation])\n",
    "    return new_text\n",
    "\n",
    "\n",
    "def text_cleaning(text):\n",
    "    new_text =  \"\".join([t for t in text if t not in string.punctuation])\n",
    "    tokens = nltk.tokenize.word_tokenize(new_text)\n",
    "    ps = nltk.PorterStemmer()\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    new_text = [ps.stem(word) for word in tokens if word not in stopwords]\n",
    "    return new_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test text_cleaning method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " photoshop , windows , office . cheap . main trending\n",
      "abasements darer prudently fortuitous undergone\n",
      "lighthearted charm orinoco taster\n",
      "railroad affluent pornographic cuvier\n",
      "irvin parkhouse blameworthy chlorophyll\n",
      "robed diagrammatic fogarty clears bayda\n",
      "inconveniencing managing represented smartness hashish\n",
      "academies shareholders unload badness\n",
      "danielson pure caffein\n",
      "spaniard chargeable levin\n",
      "\n",
      "  label                                               text  \\\n",
      "0   ham   enron methanol ; meter # : 988291\\r\\nthis is ...   \n",
      "1   ham   hpl nom for january 9 , 2001\\r\\n( see attache...   \n",
      "2   ham   neon retreat\\r\\nho ho ho , we ' re around to ...   \n",
      "3  spam   photoshop , windows , office . cheap . main t...   \n",
      "4   ham   re : indian springs\\r\\nthis deal is to book t...   \n",
      "\n",
      "                                          clean_text  \n",
      "0  [enron, methanol, meter, 988291, follow, note,...  \n",
      "1  [hpl, nom, januari, 9, 2001, see, attach, file...  \n",
      "2  [neon, retreat, ho, ho, ho, around, wonder, ti...  \n",
      "3  [photoshop, window, offic, cheap, main, trend,...  \n",
      "4  [indian, spring, deal, book, teco, pvr, revenu...  \n"
     ]
    }
   ],
   "source": [
    "print(data['text'][3])\n",
    "data['clean_text'] = data['text'].apply(lambda x: text_cleaning(x))\n",
    "print(data.head())\n",
    "#print(text_cleaning(rem_punctuations(data['text'][0:4])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#Check for null values\n",
    "\n",
    "print(data['clean_text'].isna().sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Test Split of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5132     ham\n",
       "2067    spam\n",
       "4716     ham\n",
       "4710     ham\n",
       "2268    spam\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(data[['clean_text']], data['label'], test_size=0.20, random_state=42)\n",
    "train_y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. ... 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "tfidf_vect = TfidfVectorizer(analyzer=text_cleaning)\n",
    "tfidf_vect_fit = tfidf_vect.fit(train_X['clean_text'])\n",
    "tfidf_train = tfidf_vect_fit.transform(train_X['clean_text'])\n",
    "tfidf_test = tfidf_vect_fit.transform(test_X['clean_text'])\n",
    "tfidf_train_vect = pd.DataFrame(tfidf_train.toarray())\n",
    "tfidf_test_vect = pd.DataFrame(tfidf_test.toarray())\n",
    "tfidf_train_vect.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a Logistic Regression model for Classification. Calculate fitting time and prediction time for the given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 1.978 / Predict time: 0.001 ---- Precision: 0.0 / Recall: 0.0 / Accuracy: 0.717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Viswanathan\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "import time\n",
    "start = time.time()\n",
    "logReg = LogisticRegression(random_state=10, max_iter=500, n_jobs = -1).fit(tfidf_train, train_y)\n",
    "end = time.time()\n",
    "fit_time = (end - start)\n",
    "\n",
    "start = time.time()\n",
    "y_pred = logReg.predict(tfidf_test)\n",
    "end = time.time()\n",
    "pred_time = (end - start)\n",
    "\n",
    "precision, recall, fscore, support = score(test_y, y_pred, pos_label='spam', average='binary')\n",
    "print('Fit time: {} / Predict time: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3), round((y_pred==test_y).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a Random Forest model for Classification. Calculate fitting time and prediction time for the given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "import time\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 48.563 / Predict time: 10.266 ---- Precision: 0.306 / Recall: 1.0 / Accuracy: 0.357\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "randFor = RandomForestClassifier(n_estimators=3000, max_depth=None, n_jobs = -1).fit(tfidf_train, train_y)\n",
    "end = time.time()\n",
    "fit_time = (end - start)\n",
    "\n",
    "start = time.time()\n",
    "y_pred = randFor.predict(tfidf_test)\n",
    "end = time.time()\n",
    "pred_time = (end - start)\n",
    "\n",
    "precision, recall, fscore, support = score(test_y, y_pred, pos_label='spam', average='binary')\n",
    "print('Fit time: {} / Predict time: {} ---- Precision: {} / Recall: {} / Accuracy: {}'.format(\n",
    "    round(fit_time, 3), round(pred_time, 3), round(precision, 3), round(recall, 3), round((y_pred==test_y).sum()/len(y_pred), 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
